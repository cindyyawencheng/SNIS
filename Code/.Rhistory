covDataAgg$commentsAvg = covDataAgg$comments/covDataAgg$reportEN
codexDataRawAgg = data.frame(codexDataRaw[, c('year' ,'CodexError', 'ConversionError' , 'reportEN')] %>%
group_by(year) %>% summarise_each(funs(sum(., na.rm = T))))
codexDataRawAgg$NoError = codexDataRawAgg$reportEN - codexDataRawAgg$CodexError - codexDataRawAgg$ConversionError
codexDataRawAgg2 = reshape(codexDataRawAgg,
varying = c('NoError', 'CodexError', 'ConversionError'),
v.names = c('error'), timevar = c('errorType'), times = c('NoError','CodexError', 'ConversionError'),
direction = 'long')
codexDataRawAgg2$errorType = factor(codexDataRawAgg2$errorType, levels = c('NoError','CodexError', 'ConversionError') )
codexDataRawAgg2 = codexDataRawAgg2[order(codexDataRawAgg2$errorType),]
covData2004 = covDataAgg
covData2004$year[which(covDataAgg$year<2004)] ='Before 2004'
covData2004$year[which(covDataAgg$year>=2004)] ='After 2004'
covData2004$year = factor(covData2004$year, levels = c('Before 2004', 'After 2004'))
covDataAgg2004 = data.frame(covData2004[, c('year', 'country', vars, 'attendanceShare', 'commentsAvg')] %>%
group_by(year, country) %>% summarise_each(funs(mean(., na.rm = T))))
covDataAgg2004[which(covDataAgg2004$country == 'China'), 'commentsAvg']= c(mean(covData[which(covData$country == 'China' & covData$year < 2004& covData$attendance >0), 'comments']), mean(covData[which(covData$country == 'China' & covData$year >= 2004& covData$attendance >0), 'comments']))
covDataAgg2004[which(covDataAgg2004$country == 'Vietnam'), 'commentsAvg']= c(mean(covData[which(covData$country == 'Vietnam' & covData$year < 2004& covData$attendance >0), 'comments']), mean(covData[which(covData$country == 'Vietnam' & covData$year >= 2004& covData$attendance >0), 'comments']))
## make bar graphs
# http://www.cookbook-r.com/Graphs/Bar_and_line_graphs_(ggplot2)/
#http://www.sthda.com/english/wiki/ggplot2-colors-how-to-change-colors-automatically-and-manually
## Paper graphs
setwd(pathGraphics)
pdf('attendanceShare2004.pdf')
ggplot(covDataAgg2004 [-which(covDataAgg2004 $country  %in% c('Taiwan', 'South Vietnam')),], aes(x = country, y =attendanceShare,  fill = year)) +
geom_bar(stat="identity", position=position_dodge())+
ylab('Percent of meetings attended')  +
scale_fill_manual(values=c("#999999", "#E69F00"))
dev.off()
pdf('avgNumCommentsPerMeeting2004.pdf')
ggplot(covDataAgg2004[-which(covDataAgg2004$country  %in% c('Taiwan', 'South Vietnam')),], aes(x = country, y =commentsAvg,  fill = year)) +
geom_bar(stat="identity", position=position_dodge())+
ylab('Average number of comments per meeting')+
scale_fill_manual(values=c("#999999", "#E69F00"))
dev.off()
pdf('totNumMeeting.pdf')
ggplot(codexDataRawAgg2, aes(x = year, y =error, fill = errorType)) +
geom_bar(stat="identity") + ylab('Number of Codex Meetings')
dev.off()
## Presentaiton graphs
setwd(pathGraphics)
pdf('attendanceShare2004Beamer.pdf')
ggplot(covDataAgg2004 [-which(covDataAgg2004 $country  %in% c('Taiwan', 'South Vietnam')),], aes(x = country, y =attendanceShare,  fill = year)) +
geom_bar(stat="identity", position=position_dodge())+
ylab('Percent of meetings attended')  +
xlab ('')+
scale_fill_manual(values=c("#999999", "#E69F00" ))+
theme(legend.text=element_text(size=20),legend.key.size = unit(1.5, "cm"),
axis.text.x = element_text(size = 20),
axis.title.y=element_text(size=14, vjust = 1.5))
dev.off()
pdf('avgNumCommentsPerMeeting2004Beamer.pdf')
ggplot(covDataAgg2004 [-which(covDataAgg2004 $country  %in% c('Taiwan', 'South Vietnam')),], aes(x = country, y =commentsAvg,  fill = year)) +
geom_bar(stat="identity", position=position_dodge())+
ylab('Average number of comments per meeting')+
xlab('')+
scale_fill_manual(values=c("#999999", "#E69F00"))+
theme(legend.text=element_text(size=20),legend.key.size = unit(1.5, "cm"),
axis.text.x = element_text(size = 20),
axis.title.y=element_text(size=14, vjust = 1.5))
dev.off()
### Other graphs
setwd(pathGraphics)
pdf('attendanceShare.pdf')
ggplot(covDataAgg[-which(covDataAgg$country  %in% c('Taiwan', 'South Vietnam')),], aes(x = year, y =attendanceShare,  fill = country)) +
geom_bar(stat="identity", position=position_dodge())+
ylab('Percent of meetings attended')
dev.off()
pdf('avgNumCommentsPerMeeting.pdf')
ggplot(covDataAgg[-which(covDataAgg$country  %in% c('Taiwan', 'South Vietnam')),], aes(x = year, y =commentsAvg,  fill = country)) +
geom_bar(stat="identity", position=position_dodge())+
ylab('Average number of comments per meeting')
dev.off()
install.packages('schoRsch')
install.packages('xtable')
library(schoRsch)
library(xtable)
test.C <- t.test(covData[which(covData$country == 'China' & covData$year < 2004), 'attendance'], covData[which(covData$country == 'China' & covData$year >= 2004), 'attendance'])
test.VN <- t.test(covData[which(covData$country == 'Vietnam' & covData$year < 2004), 'attendance'], covData[which(covData$country == 'Vietnam' & covData$year >= 2004), 'attendance'])
test.C.comment <- t.test(covData[which(covData$country == 'China' & covData$year < 2004& covData$attendance >0), 'comments'], covData[which(covData$country == 'China' & covData$year >= 2004& covData$attendance >0), 'comments'])
test.VN.comment <- t.test(covData[which(covData$country == 'Vietnam' & covData$year < 2004& covData$attendance >0), 'comments'], covData[which(covData$country == 'Vietnam' & covData$year >= 2004& covData$attendance >0), 'comments'])
xtable(
t_out(toutput=test.C, n.equal = TRUE, welch.df.exact = TRUE, welch.n = NA,
d.corr = TRUE, print = TRUE)
)
##################Teressract PDFs#########################
Tes.2003<- c("02%252FAL03_40e.pdf","03%252FAL03_38e","04%252FAF03_01e","05%252FAL03_11e","08%252FAL03_16e","09%252FMH03_01e","13%252Fal03_31e") #Folder 2003
if(Sys.info()['user'] == "tranganhdo"){'Users/tranganhdo/Dropbox/snis/Code')
}
setwd(pathData)
covData = read.csv('covData.csv', stringsAsFactors = F)
# This R file double checks to make sure that all files are downloaded and in the proper folders
path = '/Users/cindycheng/Dropbox/Documents/SNIS/Data/Codex/'
if(Sys.info()['user'] == "tranganhdo"){'/Users/tranganhdo/Dropbox/snis/Code')
}
setwd(path)
folders = list.files()
yearProbs = c()
n_tableProb = c()
n_folderProb = c()
n_hrefProb = c()
for ( f in 1:length(folders)){
setwd(paste(path, folders[f], sep = '/'))
table = read.csv(paste(folders[f], 'csv', sep ='.'), stringsAsFactors = F)
# checks to see all files are downloaded
n_table = sum(table$EN, table$reportEN)
href = c(table$hrefEnAgenda[grep('pdf', table$hrefEnAgenda)] , table$hrefEnReport[grep('pdf', table$hrefEnReport)])
n_href = length(href)
n_folder = length(list.files(pattern = '.pdf'))
if (n_href != n_folder){
print (paste('Discrepancy in number of files for year ', folders[f], sep = ' '))
yearProbs = c(folders[f], yearProbs)
n_tableProb = c(n_table, n_tableProb)
n_folderProb = c(n_folder, n_folderProb)
n_hrefProb = c(n_href, n_hrefProb)
} else{
print(paste('All good! -- no discrepancy in number of files for year ', folders[f], sep = ' '))
}
# checks to see that all files are in proper folder
files =  list.files(pattern = '.pdf')
nchar_files = nchar(files[1]) -1
hrefPDF = c()
for (h in 1:length(href)){
n = nchar(href[h])
hrefPDF[h] = substr(href[h], n-nchar_files , n)
}
if (length(setdiff(hrefPDF, files)) >0){
print (paste('wrong files in ', folders[f], sep = ' '))
}
else{
print (paste('no wrong files in ', folders[f], sep = ' '))
}
}
cbind(yearProbs, n_tableProb, n_folderProb, n_hrefProb)
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
setwd("/Users/tranganhdo/Dropbox/snis/Code")
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Remove punctuation
docs <- tm_map(docs, removePunctuation)
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
}
docs <- tm_map(docs, tolower)
#Removing “stopwords” (common words) that usually have no analytic value.
length(stopwords("english"))
stopwords("english")
#Remove common word endings
library(SnowballC)
docs <- tm_map(docs, stemDocument)
#Strip whitespace
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stripWhitespace)
#Tell R to treat preprocessed documents as text documents.
docs <- tm_map(docs, PlainTextDocument)
matches <- c("Vietnam", "China")
corp <- Corpus(VectorSource(docs))
for (k in 1:length(docs)){
b <- apply_as_df(docs[k], termco, match.list=matches)
a <- rbind(a, c(i,b$raw))
# we're combining the directory name and the raw word counts
# before adding the row to the data frame
}
} #done working on directories
a[1,] #print first row of output; should be 5 columns
write.csv(a, "output.csv")
}
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Remove punctuation
docs <- tm_map(docs, removePunctuation)
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
}
docs <- tm_map(docs, tolower)
#Removing “stopwords” (common words) that usually have no analytic value.
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
setwd("/Users/tranganhdo/Dropbox/snis/Code")
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Remove punctuation
docs <- tm_map(docs, removePunctuation)
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
}
docs <- tm_map(docs, tolower)
#Removing “stopwords” (common words) that usually have no analytic value.
length(stopwords("english"))
stopwords("english")
#Remove common word endings
library(SnowballC)
docs <- tm_map(docs, stemDocument)
#Strip whitespace
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stripWhitespace)
#Tell R to treat preprocessed documents as text documents.
docs <- tm_map(docs, PlainTextDocument)
matches <- c("Vietnam", "China")
corp <- Corpus(VectorSource(docs))
for (k in 1:length(docs)){
b <- apply_as_df(docs[k], termco, match.list=matches)
a <- rbind(a, c(i,b$raw))
# we're combining the directory name and the raw word counts
# before adding the row to the data frame
}
} #done working on directories
a[1,] #print first row of output; should be 5 columns
write.csv(a, "output.csv")
}
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
setwd("/Users/tranganhdo/Dropbox/snis/Code")
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Remove punctuation
docs <- tm_map(docs, removePunctuation)
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
}
docs <- tm_map(docs, tolower)
#Removing “stopwords” (common words) that usually have no analytic value.
length(stopwords("english"))
stopwords("english")
#Remove common word endings
library(SnowballC)
docs <- tm_map(docs, stemDocument)
#Strip whitespace
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stripWhitespace)
#Tell R to treat preprocessed documents as text documents.
docs <- tm_map(docs, PlainTextDocument)
matches <- c("Vietnam", "China")
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
setwd("/Users/tranganhdo/Dropbox/snis/Code")
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Remove punctuation
docs <- tm_map(docs, removePunctuation)
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
}
docs <- tm_map(docs, tolower)
#Removing “stopwords” (common words) that usually have no analytic value.
length(stopwords("english"))
stopwords("english")
#Remove common word endings
library(SnowballC)
docs <- tm_map(docs, stemDocument)
#Strip whitespace
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stripWhitespace)
#Tell R to treat preprocessed documents as text documents.
docs <- tm_map(docs, PlainTextDocument)
matches <- c("Vietnam", "China")
corp <- Corpus(VectorSource(docs))
for (k in 1:length(docs)){
b <- apply_as_df(docs[k], termco, match.list=matches)
a <- rbind(a, c(i,b$raw))
# we're combining the directory name and the raw word counts
# before adding the row to the data frame
}
} #done working on directories
a[1,] #print first row of output; should be 5 columns
write.csv(a, "output.csv")
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
setwd("/Users/tranganhdo/Dropbox/snis/Code")
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
setwd("/Users/tranganhdo/Dropbox/snis/Code")
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Remove punctuation
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Remove punctuation
docs <- tm_map(docs, removePunctuation)
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
}
docs <- tm_map(docs, tolower)
#Removing “stopwords” (common words) that usually have no analytic value.
length(stopwords("english"))
stopwords("english")
#Remove common word endings
library(SnowballC)
docs <- tm_map(docs, stemDocument)
#Strip whitespace
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stripWhitespace)
#Tell R to treat preprocessed documents as text documents.
docs <- tm_map(docs, PlainTextDocument)
matches <- c("Vietnam", "China")
corp <- Corpus(VectorSource(docs))
for (k in 1:length(docs)){
b <- apply_as_df(docs[k], termco, match.list=matches)
a <- rbind(a, c(i,b$raw))
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Remove punctuation
docs <- tm_map(docs, removePunctuation)
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
}
docs <- tm_map(docs, tolower)
#Removing “stopwords” (common words) that usually have no analytic value.
length(stopwords("english"))
stopwords("english")
#Remove common word endings
library(SnowballC)
docs <- tm_map(docs, stemDocument)
#Strip whitespace
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stripWhitespace)
#Tell R to treat preprocessed documents as text documents.
docs <- tm_map(docs, PlainTextDocument)
matches <- c("Vietnam", "China")
corp <- Corpus(VectorSource(docs))
for (k in 1:length(docs)){
b <- apply_as_df(docs[k], termco, match.list=matches)
a <- rbind(a, c(i,b$raw))
# we're combining the directory name and the raw word counts
# before adding the row to the data frame
}
} #done working on directories
a[1,] #print first row of output; should be 5 columns
write.csv(a, "output.csv")
}
library('tm')
install.packages('tm')
library('tm')
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
ysetwd("/Users/tranganhdo/Dropbox/snis/Code")
# get list of directories inside the currect working directory
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
# Remove punctuation
docs <- tm_map(docs, removePunctuation)
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
}
docs <- tm_map(docs, tolower)
#Removing “stopwords” (common words) that usually have no analytic value.
length(stopwords("english"))
stopwords("english")
#Remove common word endings
library(SnowballC)
docs <- tm_map(docs, stemDocument)
#Strip whitespace
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stripWhitespace)
#Tell R to treat preprocessed documents as text documents.
docs <- tm_map(docs, PlainTextDocument)
matches <- c("Vietnam", "China")
corp <- Corpus(VectorSource(docs))
for (k in 1:length(docs)){
b <- apply_as_df(docs[k], termco, match.list=matches)
a <- rbind(a, c(i,b$raw))
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "qdap")
install.packages(Needed, dependencies=TRUE)
# get list of directories inside the currect working directory
directories <- list.dirs(path = ".", full.names = FALSE)
# set up the data frame that will become the .csv
a <- data.frame(dirname=c(), docs=c(), word.count=c(), Vietnam=c(), China=c())
# iterate through each directory, looking for texts
for (i in directories){
if (nchar(i) > 0) {           #skip the current directory
cname <- file.path("~", "Dropbox", "test", i)
dir(cname)   # Use this to check to see that your texts have loaded.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
